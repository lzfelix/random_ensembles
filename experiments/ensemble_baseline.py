import argparse
from typing import Tuple
import numpy as np

from misc import utils
from models import textio
from models import ensemble


def get_exec_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(usage='Compute uniform (1/k) and majority-voting baseline ensemble strategies')
    parser.add_argument('ens_method', help='Ensemble method: majority or uniform (for 1/k)', type=str)
    parser.add_argument('val_ground', help='Path to the validation ground truth labels', type=str)
    parser.add_argument('tst_ground', help='Path to the test ground truth labels', type=str)
    parser.add_argument('-val_preds', help='List of candidate predictions in the validation set', type=str, nargs='+',
                        required=True)
    parser.add_argument('--show_test', help='Shows model accuracy @ train test in the end summary', action='store_true')

    return parser.parse_args()


def read_file(filepath: str) -> Tuple[float, np.ndarray]:
    """Read a prediction test file generated by a random/fine-tuned weak model"""
    with open(filepath) as dfile:
        rows = dfile.readlines()

    n_rows, accuracy = rows[0].split()
    n_rows = int(n_rows)
    individual_accuracy = float(accuracy)

    # From rows 1 until the end of file, each line contains C numbers,
    # where C is the number of classes for the problem and each of these
    # numbers is the `log(softmax)` of i-th sample (i-th line) belonging
    # to the j-th class (j-th column). Since these are logs, it's possible to
    # get `softmax(x) = exp(log(softmax(x))`.
    logits = [[float(p) for p in predictions.split()] for predictions in rows[1:]]

    logits_matrix = np.asarray(logits)
    assert logits_matrix.shape[0] == n_rows,\
        f'There are only {logits_matrix.shape[0]} rows on file {filepath}. Expected {n_rows}'

    return individual_accuracy, np.exp(logits_matrix)


if __name__ == '__main__':
    exec_args = get_exec_args()
    ensemble_method = exec_args.ens_method
    assert ensemble_method in ['majority', 'uniform'], 'ens_method should be either "majority" or "uniform"'

    tst_preds = textio.deduce_test_files(exec_args.val_preds)

    textio.show_files_list('validation', exec_args.val_ground, exec_args.val_preds)
    textio.show_files_list('test', exec_args.tst_ground, tst_preds)
    print(exec_args)

    val_all_preds, val_y_true = ensemble.load_candidates_preds(exec_args.val_preds, exec_args.val_ground)
    tst_all_preds, tst_y_true = ensemble.load_candidates_preds(tst_preds, exec_args.tst_ground)

    if ensemble_method == 'majority':
        print('\nEnsemble strategy: majority voting\n')
        val_accuracy = ensemble.evaluate_majority_voting(val_all_preds, val_y_true)
        tst_accuracy = ensemble.evaluate_majority_voting(tst_all_preds, tst_y_true)
    else:
        print('\nEnsemble strategy: 1/k voting\n')
        n_models = val_all_preds.shape[0]
        weights = np.ones([n_models]) / n_models # All models have the same weight: 1/k

        val_accuracy = ensemble.evaluate_ensemble(weights, val_all_preds, val_y_true)
        tst_accuracy = ensemble.evaluate_ensemble(weights, tst_all_preds, tst_y_true)

    print('Individual val. model accuracies')
    print('{:10} {:10} {:10}'.format('Model ID', 'acc @ val', 'acc @ tst'))
    print('-' * 33)
    for i, (val_pred, tst_pred) in enumerate(zip(val_all_preds, tst_all_preds)):
        val_acc = utils.accuracy(val_pred, val_y_true)
        tst_acc = utils.accuracy(tst_pred, tst_y_true) if exec_args.show_test else '????'
        print(f'{i:<10} {val_acc:>10.4}{tst_acc:>10.4}')

    tst_accuracy = tst_accuracy if exec_args.show_test else '????'
    print(f'\nEnsemble val. accuracy: {val_accuracy}')
    print(f'Ensemble tst. accuracy: {tst_accuracy}')
